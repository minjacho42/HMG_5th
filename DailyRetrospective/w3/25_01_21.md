# 데일리 리뷰

날짜: 2025년 1월 21일
작성자: 최민제

# ✏️ Review

## Hadoop Single Node Cluster on Docker

### Configuration을 위한 xml파일들 정리

- `core-site.xml`
    
    ```xml
    <configuration>
        <!-- HDFS의 기본 파일 시스템 주소 -->
        <!-- 0.0.0.0으로 모든 ip 주소에서 접근 가능하도록 변경 -->
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://0.0.0.0:9000</value>
        </property>
    
        <!-- 웹에서 데이터를 업로드할 때 필요. -->
        <!-- Docker에서 container 간의 통신을 진행할 경우에 필요.-->
        <property>
            <name>dfs.client.use.datanode.hostname</name>
            <value>true</value>
        </property>
    
        <property>
            <name>dfs.datanode.hostname</name>
            <value>localhost</value>
        </property>
    
    </configuration>
    ```
    
- `hdfs-site.xml`
    
    ```xml
    <configuration>
        <!-- 블록 복제본 수 (단일 노드에서는 1로 설정) -->
        <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>
    
        <!-- 네임노드 데이터 저장 디렉토리 -->
        <property>
            <name>dfs.namenode.name.dir</name>
            <value>file:///hadoop/dfs/name</value>
        </property>
    
        <!-- 데이터노드 데이터 저장 디렉토리 -->
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>file:///hadoop/dfs/data</value>
        </property>
    
        <!-- Web으로 hdfs 접근 가능하게 설정 -->
        <property>
            <name>dfs.webhdfs.enabled</name>
            <value>true</value>
        </property>
    
        <!-- Web으로 hdfs 접근 가능하게 설정 -->
        <property>
            <name>dfs.namenode.http-address</name>
            <value>0.0.0.0:9870</value>
        </property>
    
        <!-- Web으로 hdfs 접근 가능하게 설정 -->
        <property>
            <name>dfs.datanode.http.address</name>
            <value>0.0.0.0:9864</value>
        </property>
    
        <!-- Web에서 접근하는 사용자들도 업로드 가능하게 변경 -->
        <property>
            <name>dfs.permissions.enabled</name>
            <value>false</value>
        </property>
    </configuration>
    ```
    
- `start-dfs.sh` 를 이용해서 hdfs가 실행된다.

### Multinode 실행을 위한 workers 파일

- master node에서는 workers 파일을 통해서 master node의 [start-dfs.sh](http://start-dfs.sh)을 실행하는 것으로 한번에 다른 컨테이너에 data node를 종속적으로 만들어주게 된다.

# 🤔 Retrospective

## ⚠️ Problem

- 과제에 매몰되어서 계속해서 근시안적인 사고를 가지고 코드를 작성하였다. 문제가 계속해서 해결되지 않으면 마음을 가다듬고 좀 더 넓은 시야로 코드를 다시 보도록 하자!!!
- 선택과 집중이 필요한 시기이다. 사분면을 잘 활용하여서 어떠한 부분이 더 중요한지 생각하여 일정을 짜고 해당 일정에 맞게 학습을 진행하자.

## 🌟 Keep

## 💡 Try