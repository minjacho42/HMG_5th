# 데일리 리뷰

날짜: 2025년 2월 12일

# ✏️ Review

## 데이터 파이프라인에 대한 고민

1. 어디까지가 Extract이고 어디부터 Transform인가?
    - Extract는 말그대로 RAW DATA를 가져오는데에 그쳐야된다고 생각하였다.
        
        → HTML 파일을 request로 보내고, metadata (request를 보낸 url)과 함께 json으로 저장하였다.
        
    - 각 파일을 하나의 HTML 파일로 저장하는 것이 맞는 것인가? 너무 큰 데이터가 보관되는 것이 아닌가?
        
        → s3의 특성상 하나의 큰 파일을 저장하게 되면, 이후에 에러처리를 통해서 해당 파일을 업데이트하는 방식이 작동할 수 없기에, 각 request의 결과를 하나의 파일로 저장하는 방식을 택했다.
        
        → 즉, 1번부터 10번까지의 html 파일이 있을 때, 8번 html에 대한 request만 실패한 경우, 해당 파일을 합쳐서 저장하게 되면, 8번을 제외한 1부터 10까지가 합쳐진 파일이 저장될 것이고, 이는 이후 통일성을 위해서 8번을 재로딩하게 되면, 다시 s3에서 해당 파일을 읽어와서 원래의 내용을 수정한 이후에 다시 재저장하는 방식이 필요하고, 이러한 부분이 비효율적일 것이라고 생각했다.
        
        → 빅 데이터적인 관점에서는 게시물 하나가 없다고해도 큰 타격이 아니지 않을까?
        
    - 각 파일을 Daily batch로 이후에 I/O로 불러오게 되면 큰 부담이 되지 않을까?
        - 해당 문제는 해결할 필요가 있다. 1000개의 작은 파일을 개별로 불러오는 것 보다는 하나의 큰 파일을 한번에 불러오는 것이, spark가 좀 더 효율적으로 처리할 수 있지 않을까 생각한다.
        - 주기적으로 작은 파일들을 하나로 묶는다던지 여러가지 방안을 고려할 필요가 있다.
2. parsing을 어디서 할 것인가?
    1. 만약에 파싱을 람다에서 개별로 하게되는 경우, lambda를 각 파일별로 부르거나, 람다 내부에서 멀티 스레딩 환경을 구성하는 방식으로 여러 파일을 동시에 처리하는 방식을 진행해야되며, 이러한 부분이 비효율적이라고 생각된다.
    2. 만약에 파싱을 spark를 통해 진행하게되면, bs4를 EMR위에 설치하는 추가 설치가 발생하고, 각 노드도 무거워질 것이라는 우려가 있다.
    
    → 계속해서 고민해볼 필요성이 있다. 
    

# 🤔 Retrospective

## ⚠️ Problem

- 너무 계속해서 뜬구름을 잡는 내용으로 토론을 하였다. 시간이 부족한 만큼 빠르게 코드를 작성하면서 결과를 확인하고 피드백할 필요가 있다.
- “이러지 않을까요?” 라는 내용의 토론이 있었다. 확실한 내용을 기반으로 어떻게 할지를 생각해야된다.

## 🌟 Keep

## 💡 Try