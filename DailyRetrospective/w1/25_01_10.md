# 데일리 리뷰

날짜: 2025년 1월 10일
작성자: 최민제

# ✏️ Review

## 📔 강의

## 🧑‍💻 과제

### `asyncio`, `aiohttp`

- **`asyncio`**
    - [`asyncio.gather()`](https://docs.aiohttp.org/en/stable/client_reference.html#aiohttp.ClientResponse) 는 동시에 awaitable한 객체들을 실행시키게 된다.
        
        `return_exceptions=True`를 설정하게 되면, return받은 result 중 exception이 있는지 체크하여 에러처리가 가능하다.
        
        `return_exceptions=False`인 경우에는, 어떠한 코루틴에서 exception이 발생한 즉시 해당 exception이 raise되기에 다른 코루틴의 결과를 확인할 수 없게 된다.
        
- **[`aiohttp`](https://docs.aiohttp.org/en/stable/)**
    - [`aiohttp.ClientSession()`](https://docs.aiohttp.org/en/stable/client_reference.html#aiohttp.ClientSession) 으로 session을 만들고 request를 보낼 수 있게 된다.
    - [`ClientSession.get()`](https://docs.aiohttp.org/en/stable/client_reference.html#aiohttp.ClientSession.get) 으로 만들어진 client session을 통해서 get request를 보내게 된다. 이후 return값은 ClientResponse 객체가 된다.
    - [`ClientResponse.json()`](https://docs.aiohttp.org/en/stable/client_reference.html#aiohttp.ClientResponse) 을 통해서 response 객체로 받은 결과를 json으로 read한다.

### `abc`

- 추상 클래스를 만들기 위해서 필요한 파이썬 라이브러리이다.
- `from abc import *` 을 진행한 이후에 decorater @abstractmethod로 추상함수를 만들 수 있다.
- 이때, 만들려는 abstract class는 class ABC를 상속받아야 된다.

### Extractor 객체

- W1M3에서 request를 여러 방식으로 보내면서, base API와 해당 API에서 얻어올 여러 endpoint의 정보를 raw json file로 저장하는 객체를 충분히 만들 수 있을 것이라는 생각을 하였다.
- `class Extractor(ABC)` 를 통해서 Abstract Base Class를 만들었으며, 해당 class에  `run()` , `save()` 두개의 abstract function을 만들었다.
- `class ExtractorWithWeb(Extractor)` 는 `run()`을 하게 되면, 생성자에서 받은 file_path에 url, max_tries 그리고 timeout을 이용하여 비동기적인 request로 web scrapping을 하게 된다.
    
    만약에 max try의 횟수가 지나도록 계속해서 성공적으로 scrapping을 할 수 없게 되면, raw json file의 `meta_data` 내부 `broken` 필드가 `True` 로 활성화 되게 된다. 그게 아니라면, raw json file의 `data` 필드에 웹 스크래핑된 결과가 저장되게 된다.
    
- `class ExtractorWithAPI(Extractor)` 의 경우, `ExtractorWithWeb` 과 비슷하게 file_path, base_url, max_tries 그리고 timeout을 생성자에서 받게 되며, 추가적으로 endpoints에 request를 보낼 모든 endpoint의 list를 받게 된다.
    
    `run()` 을 하게 되면, 비동기적으로 모든 endpoint에 request를 보내게 되며, 실패한 endpoint에 대해서는 max try 만큼 추가적으로 request를 시도한다.
    
    일부만 성공하더라도, 이후에 해당 부분의 raw data가 필요할 수 있기에, json 파일의 data 필드에는 `(end point, request result)` 같은 형태로 결과가 저장되며, 실패한 end point들은 meta data의 broken_endpoints에 기록되게 된다.
    
    이는 이후 raw data를 기반으로 일부 request만 재시도 할  수 있는 기회를 주게 된다.
    

### Extractor 객체 적용 결과

- 객체를 적용한 결과, api를 통한 request에서는 2초 가량의 차이가 나는 것을 확인할 수 있었다.
    
    ![25_1_10_1.png](https://github.com/minjacho42/HMG_5th/blob/master/DailyRetrospective/srcs/25_1_10_1.png)
    

## 기타

- Extract는 언제 변경해야 할까?
    - Extract 할 데이터의 소스가 변경되었을 때.
    - Extract 할 외부 소스에 제약 사항이 추가되었을 때.
- Transform은 언제 변경해야 할까?
    - Extract한 RAW data의 내부 구조가 변경되었을 때.
    - Load하려는 데이터의 목적이나 요구사항이 변경되었을 때.
- Load는 언제 변경해야 할까?
    - Load하려는 데이터의 목적이나 요구사항이 변경되었을 때.
        - column의 type이 변경되었을 때
        - column의 단위가 변경되었을 때
        - column을 추가하거나 삭제할 필요가 있을 때.
- i.e. wikipedia 웹페이지가 변경되면 뭐를 바꿔야 할까요? Extract만 바꿔주면 TL은 안바꿔도 되도록 코드가 작성되어 있나요?
- i.e. 100B 단위로 GDP를 보고 싶다고 하면 뭐를 바꿔야 할까요? Transform만 바꿔주면 되나요?
- i.e. 저장하고자 하는 데이터의 종류가 늘어나거나 줄어들면 Transform, Load 둘다 바꿔야만 하나요?

# 🤔 Retrospective

## ⚠️ Problem

## 🌟 Keep

- 주석을 달 때, 영어로 된 비문을 유지하는 것이 아니라 chatgpt를 통해서 작성한 주석이 비문인지 아닌지 한번 체크한 이후에 변환하는 작업을 하였다.
    
    → 이후에 주석을 다시 확인할 때 좀 더 가독성 면에서 좋은 방법이라고 생각한다. 계속해서 유지할 필요가 있다.
    

## 💡 Try