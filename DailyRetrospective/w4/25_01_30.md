# 데일리 리뷰

날짜: 2025년 1월 30일
작성자: 최민제

# ✏️ Review

## pyspark.sql 관련 함수

- [`agg()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.agg.html#pyspark.sql.DataFrame.agg)
    
    ```python
      nyc_taxi_hourly_summary = nyc_taxi_hourly_df.groupBy(
          'pickup_date_hour'
      ).agg(
          count(col('tpep_pickup_datetime')).alias('trip_count'),
          avg(col('trip_distance')).alias('avg_trip_distance'),
          avg(col('total_amount')).alias('avg_total_amount'),
          avg(col('trip_min')).alias('avg_trip_min')
      )
    ```
    
    위 코드는, 하나의 column 값을 기반으로 groupby를 한 이후에, 각 group 별, 평균값이나, row 개수를 구하게 된다.
    
- zscore를 통한 필터링
    
    ```python
    # 이상치를 제거하는 함수. 표준점수가 3.5이상 넘어가면 필터링된다.
    def remove_outliers_zscore(df, columns, threshold=3.5):
        stats = df.select(
            *[mean(col(c)).alias(f"{c}_mean") for c in columns],
            *[stddev(col(c)).alias(f"{c}_stddev") for c in columns]
        ).collect()[0]
    
        for c in columns:
            mean_val = stats[f"{c}_mean"]
            stddev_val = stats[f"{c}_stddev"]
            df = df.filter(((col(c) - mean_val) / stddev_val).between(-threshold, threshold))
    
        return df
    ```
    
    df.select() 내부에서 column으로 주어진 값들에 대해서 각각의 mean과 stddev를 구한다.
    
    z-score를 통해서 데이터가 평균에서 3.5이상 넘어가게 되면, 값을 제거하도록 하였다.
    

## 팀 프로젝트 관련

- 웹툰 데이터를 크롤링 한 이후에, 피드백을 통한 개선을 위해서 독자들의 댓글이 긍정적이 반응을 가졌는지 부정적인 반응을 가졌는지 확인할 필요가 있었다. 이를 위해, 감성분석에 적합한 모델을 찾아보았고, 결과적으로 koelectra를 쇼핑몰 리뷰를 통해 추가 파인튜닝한 모델을 활용하였다.
    
    이후에는 데이터를 모아서 직접 라벨링을 한다면 좀 더 좋은 성능을 낼 수 있을 것이라고 생각하며, 현재 미니 프로젝트에서의 실현 가능성을 파악할 수 있는 정도로의 모델로는 적합하다고 판단하여 해당 모델을 사용하였다.
    

# 🤔 Retrospective

## ⚠️ Problem

- 출력 저장 형식을 좀 더 개선 할 수 있는 부분을 일단 파악하였지만, 넘어갔다.
    - csv가 parquet에 비해서 File I/O 부분에서 비효율적이라는 것을 확인하였지만, 시간이 부족하다는 이유로 개선하지 못했다.

## 🌟 Keep

## 💡 Try